%% Time-stamp: <2019-01-15 14:22:02 (marc)>
\documentclass[xcolor=x11names,compress, mathserif]{beamer}

\newcommand\hmmax{0}
\newcommand\bmmax{0}

\usepackage{../includes/MarkMathCmds}

\newcommand{\hackspace}{\hspace{4.2mm}}
\newcommand{\showstudent}[1]{}





% talk/author information
\newcommand{\authorname}{Mark van der Wilk}
\newcommand{\authoremail}{m.vdwilk@imperial.ac.uk}
\newcommand{\authoraffiliation}{
 Department of Computing\\Imperial
  College London}
\newcommand{\authortwitter}{markvanderwilk}
\newcommand{\slidesettitle}{\imperialBlue{Graphical Models}}
\newcommand{\footertitle}{Graphical Models}
\newcommand{\location}{Imperial College London}
\newcommand{\talkDate}{{January 16, 2022}}



\date{\imperialGray{\talkDate}}




% load defaults
\input{../includes/header.tex}
\input{../includes/titlepage.tex}

\linespread{1.2} 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%s
\begin{frame}{Probabilistic Models}

%   \begin{itemize}
%     \item Quantity of interest: Joint distribution $p(\vec x, \vec z)
%       =  p(\vec z) p(\vec x|\vec z)$
%       of all observed $\vec x$ and
%       unobserved (latent) $\vec z$ random variables \\
%       \arrow \cemph{Probabilistic model}
%     \item Comprises information about the prior, the likelihood and
%       the posterior
% \pause 
%     \item Joint distribution $p(\vec x, \vec z)$  itself can be complicated
%     \item \calert{Does not tell us anything about structural properties
%         of the probabilistic model} (e.g., factorization, independence)
%     \end{itemize}
%   \arrow \emph{Probabilistic graphical models} 


  Previously we saw:
  \begin{itemize}
    \item Probabilistic model is a \emph{joint distribution} $p(\vx, \vz)$.
    \item We make factorisation assumptions to specify the model.
    \item Factorisation assumptions help simplify the posterior.
  \end{itemize}

  \vspace{0.5cm}

  Graphical models help us to:
  \begin{itemize}
    \item visualise (conditional) independence,
    \item specify models with the right structure,
    \item find (conditional) independence when conditioning,
    \item do inference automatically and efficiently.
  \end{itemize}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Probabilistic Graphical Models}

\begin{figure}
\centering
\includegraphics[height = 2cm]{./figures/bayesnet}
\hfill
\includegraphics[height = 2cm]{./figures/mrf}
\hfill
\includegraphics[height = 2cm]{./figures/factor_graph}
\end{figure}

Three types of probabilistic graphical models:
\begin{itemize}
\item \cemph{Bayesian networks (directed graphical models)}
\item \cemph{Markov random fields (undirected graphical models)}
\item \cemph{Factor graphs}
\end{itemize}
\pause
\begin{itemize}
\item \textbf{Nodes:} (Sets of) random variables
\item \textbf{Edges:} Probabilistic/functional relations between variables
\end{itemize}

\pause
\arrow Graph captures the \cemph{way in which the joint distribution over all
random variables can be decomposed} into a product of factors depending
only on a subset of these variables

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}
% \frametitle{Why are they useful?}
% \begin{itemize}[<+->]
% \item Simple way to \cemph{visualize the structure} of a probabilistic
%   model
% \item \cemph{Insights into properties} of the model (e.g., conditional
%   independence) by inspection of the graph
% \item Can be used to \cemph{design/motivate new models}
% \item Complex computations for inference and learning can be expressed
%   in terms of \cemph{graphical manipulations} 
% \end{itemize}
% \end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Importance of Visualization}
\vspace{-5mm}
\begin{figure}
\centering
\includegraphics[width = 0.7\hsize]{./figures/ex1b}\\
\pause
\includegraphics[height = 4.7cm]{./figures/ex1}\nocite{Kim2015}
\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{}
\begin{center}
{\Large \emph{Expressing Factorisations as Graphs}} \\

\end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Directed Graphical Models}
  \begin{figure}
    \includegraphics[height = 4cm]{./figures/d-separation1}
  \end{figure}
  \begin{itemize}
  \item Nodes: Random variables
  \item Shaded nodes: Observed random variables
  \item Unshaded nodes: Unobserved (latent) random variables
    \item Directed arrow from $a$ to $b$: Conditional distribution
      $p(b|a)$. 
  \end{itemize}
  
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{frame}
\frametitle{Skill: From Joints to Graphs}
Consider the joint distribution
\begin{align*}
p(a,b,c) = p(c|a,b)p(b|a)p(a)
\end{align*}

Building the corresponding graphical model:
\begin{enumerate}[<+->]
\item Create a node for all random variables
\item For each conditional distribution, we add a directed link (arrow) to
the graph from the nodes corresponding to the variables on which the
distribution is conditioned on
\end{enumerate}

\begin{figure}
\centering
\includegraphics[height = 2cm]{./figures/bayesnet2}
\end{figure}
\onslide+<3->{
\arrow Graph layout depends on the choice of factorization
}
\end{frame}



% %%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{Skill: From Graphs to Joints}
\begin{figure}
\centering
\includegraphics[height=3cm]{./figures/bayesnet3}
\end{figure}
\begin{itemize}
\item \cemph{Joint distribution is the product of a set of conditionals}, one
  for each node in the graph
  \pause
\item Each conditional depends only on the \cemph{parents of the
corresponding node} in the graph
\end{itemize}
\pause
\vspace{-2mm}
\begin{align*}
p(x_1,x_2,x_3,x_4,x_5) = p(x_1)p(x_5)p(x_2|x_5)p(x_3|x_1,x_2)p(x_4|x_2)
\end{align*}
\pause
In general:\qquad
$
\colchar{$p(\vec x) = p(x_1, \dotsc, x_K) = \prod_{k = 1}^Kp(x_k|\text{parents}(x_k))$}{blue}
$

\end{frame}


\begin{frame}{What is \emph{the} graphical model?}
Remember, a model is defined simply by its joint distribution, which often is just between data and a latent variable:
\begin{align}
p(\vx, \vz)
\end{align} \pause
We can factorise this in two distinct, but equally valid ways:
\begin{align}
p(\vx, \vz) = p(\vx|\vz)p(\vz) = p(\vx) p(\vz|\vx)
\end{align}\pause
\vspace{-0.3cm}
\begin{columns}
\column{0.5\hsize}
\centering
      \tikz{ %
        \node[latent] (x) {$\vx$} ; %
        \node[latent, left=of x] (z) {$\vz$} ; %
        \edge {x} {z} ; %
      }
\column{0.5\hsize}
\centering
      \tikz{ %
        \node[latent] (x) {$\vx$} ; %
        \node[latent, left=of x] (z) {$\vz$} ; %
        \edge {z} {x} ; %
      }
\end{columns}
\vspace{0.3cm}

Which one is correct? \pause Depends on which conditional you specified!
\begin{align}
p(\vx, \vz) = \colchar{$p(\vx|\vz)$}{green}\colchar{$p(\vz)$}{green} = \colchar{$p(\vx)$}{red} \colchar{$p(\vz|\vx)$}{red}
\end{align}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Graphical Model for (Bayesian) Linear Regression}

\begin{columns}
\column{0.5\hsize}
\begin{figure}
\centering
\includegraphics[width = \hsize]{./figures/Figure1_2}\\
\tiny{From PRML (Bishop, 2006)\nocite{Bishop2006}}
\end{figure}
\column{0.5\hsize}
We are given a data set $(x_1,y_1), \dotsc, (x_N,y_N)$ where
\begin{align*}
y_i = f(x_i) + \varepsilon, \quad \varepsilon\sim\gauss{0}{\sigma^2}
\end{align*}
with $f$ unknown. \\
\arrow Find a (regression) model that explains the data
\end{columns}

\pause
\begin{itemize}
\item Consider \cemph{polynomials}
$
f(x) = \sum_{j=0}^M w_j x^j
$
with parameters $\vec w = [w_0,\dotsc,w_M]\T$. 
\item \cemph{Bayesian linear regression:} Place a conjugate Gaussian prior on
  the parameters: $p(\vec w) = \gauss{\vec 0}{\alpha^2\mat I}$
\end{itemize}


\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}[t]
\frametitle{Graphical Model for Linear Regression}

\begin{columns}
\column{0.35\hsize}
\begin{figure}
\includegraphics[width =\hsize]{./figures/Figure1_2}\\
\tiny{From PRML (Bishop, 2006)}
\end{figure}
\column{0.5\hsize}
\begin{align*}
p(y_i|\vec w,x_i) &= \gaussx{y_i}{f_{\vec w}(x_i)}{\sigma^2}\\
%\varepsilon &\sim\gauss{0}{\sigma^2}\\
f(x) &=\sum_{j=0}^M w_j x^j\\
p(\vec w) &=\gauss{\vec 0}{ \alpha^2\mat I}
\end{align*}
\end{columns} \pause
\begin{figure}
\centering
\includegraphics[height = 3cm]{./figures/gm_bayesian_regression1}
\hfill
\pause
\includegraphics[height =
3cm]{./figures/gm_plate_bayesian_regression1}
\hfill
\pause
\includegraphics[height = 3cm]{./figures/gm_plate_bayesian_regression2}
\end{figure}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{frame}
\frametitle{}
\begin{center}
{\Large \emph{Finding Conditional Independence}} \\
\end{center}
\end{frame}




\begin{frame}
\frametitle{Conditional Independence}
% \begin{figure}
% \centering
% \includegraphics[height = 2cm]{./figures/bayesnet}
% \end{figure}
\begin{center}
      \tikz{ %
        \node[latent] (a) {$a$} ; %
        \node[latent, right=of a] (b) {$b$} ; %
        \node[latent, above=of a] (c) {$c$} ;
%        \node[latent, left=of x] (z) {$\vz$} ; %
        \edge {c} {a} ; %
        \edge {c} {b} ; %
      }
\end{center}
\begin{align*}
  a \ci b | c &\iff p(a,b|c) = p(a|c)p(b|c) \\
  &\iff p(a|b,c) = p(a|c)
\end{align*}

\vspace{-0.3cm}

\begin{itemize}
\item (Conditional) independence allows for a \cemph{factorization of the
  joint distribution} \arrow More efficient inference
\begin{align}
\Exp{p(a,b|c)}{f(a)g(b)} = \Exp{p(a|c)}{f(a)}\cdot\Exp{p(b|c)}{g(b)}
\end{align}
\item \cemph{Conditional independence} properties of the joint
  distribution can be read directly from the graph without analytical manipulations!
\end{itemize}
\arrow \emph{d-separation} (Pearl, 1988\nocite{Pearl1988})

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{D-Separation (Directed Graphs)}

\begin{columns}
\column{0.4\hsize}
\begin{figure}
\centering
\includegraphics[height = 3cm]{./figures/d-separation}
\end{figure}
\column{0.6\hsize} Directed, acyclic graph in which $A,B,C$ are
arbitrary, non-intersecting sets of nodes. Does $A\ci B|C$
hold?\newline Note: $C$ is observed if we condition on it (and the
nodes in the GM are shaded)
\end{columns}
\pause
\vspace{0.5mm}
\arrow Consider \cemph{all possible paths} from any node in $A$ to any node in
$B$.
\pause
\newline Any such \emph{path is blocked} if it includes a node
such that either
\begin{itemize}
\item Arrows on the path meet either \cemph{head-to-tail or
    tail-to-tail} at the node, \underline{and} the node is in the set
  $C$ or
  \pause
\item Arrows meet \cemph{head-to-head} at the node \underline{and}
  neither the node nor any of its descendants is in the set $C$
\end{itemize}
\pause
If \cemph{all paths are blocked}, then $A$ is \cemph{d-separated}
(conditionally indep.) from
$B$ by $C$, and the joint distribution satisfies $A\ci B|C$.


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Exam skill: Find conditional independencies}
\begin{figure}
\centering
\subfloat[$a\ci b|c$?]{
\includegraphics[height = 3cm]{./figures/d-separation1}
% unblocked (arrows meet head-to-head in e, and c (descendant of e) is observed
}
\hspace{1cm}
\subfloat[$a\ci b|d$?]{
\includegraphics[height = 3cm]{./figures/d-separation2}
% blocked for 2 reasons:
% (1) d has tail-to-tail arrows and is observed
% (2) e has head-to-head arrows and neither e nor its descendants are observed
}
\end{figure}
\begin{myblock}{}
A path is \emph{blocked} if it includes a node such that either
\begin{itemize}
\item The arrows on the path meet either head-to-tail or tail-to-tail at the
node, and the node is in the set $C$ (observed) or
\item The arrows meet head-to-head at the node, and neither the node nor any
of its descendants is in the set $C$ (observed) 
\end{itemize}
\end{myblock}
\end{frame}



\begin{frame}{}
\begin{center}
\Large \emph{(Hidden) Markov Models}
\end{center}
\begin{itemize}
\item Models of time-varying phenomena (time series)
\item Conditional Independencies are crucial
\end{itemize}
\end{frame}


\begin{frame}{Time-series models}
Many phenomena vary over time:
\begin{itemize}
\item Dynamical systems
\begin{itemize}
\item motion of stars in the sky, that you want to understand
\item motion of a robot arm, that you want to control
\end{itemize}
\item Audio signals
\item Communication signals
\item Price of assets over time
\end{itemize}

\vspace{0.5cm}

\pause
More generally, we can think about \emph{sequence} models:
\begin{itemize}
\item DNA sequences
\item Language
\end{itemize}
\end{frame}



\begin{frame}{Time-series models}
What are the relevant random variables? \pause \\
At the very least, we have one observation for each time point, and a joint over all:
\begin{align}
p(x_1, x_2, x_3, x_4, \dots, x_T)
\end{align} \pause
\begin{itemize}
\item To simplify the specification of the model, we can \emph{assume} that $x_t$ is the \emph{state} of the dynamical system. \pause
\item The state is a variable that, if known, determines \emph{everything} there is to know about future states. \pause
\item In other words, $x_{t+T} \ci x_{t-\tau} | x_t$ for $t,T > 0$.
\end{itemize}
\end{frame}


\begin{frame}{Markov Chains}
We can express this assumption with the factorisation:
\begin{align}
p(x_1, \dots, x_5) = p(x_1) \prod_{t=2}^5 p(x_t|x_{t-1})
\end{align}
With the corresponding graphical model:
\begin{center}
\scalebox{0.7}{
      \tikz{ %
        \node[latent] (x1) {$x_1$} ; %
        \node[latent, right=of x1] (x2) {$x_2$} ; %
        \node[latent, right=of x2] (x3) {$x_3$} ; %
        \node[latent, right=of x3] (x4) {$x_4$} ; %
        \node[latent, right=of x4] (x5) {$x_5$} ; %
        \edge {x1} {x2} ; %
        \edge {x2} {x3} ; %
        \edge {x3} {x4} ; %
        \edge {x4} {x5} ; %
      }
}
\end{center} \pause

\vspace{0.3cm}

Exercise: Show that $x_4 \ci x_1 | x_3$. \pause \\
\arrow Single path from $x_4$ to $x_3$, blocked by rule 1 since $x_3$ is observed.
\end{frame}


\begin{frame}{Hidden Markov Models}
\begin{itemize}
\item Most interesting time series models only have \emph{indirect} observations about the state. \pause
\item We observe the state $x_t$ with some extra randomness through some distribution $p(y_t|x_t)$. \pause
\item This hides the state from direct view: Hidden Markov Model:
\end{itemize}
\begin{center}
\scalebox{0.7}{
      \tikz{ %
        \node[latent] (x1) {$x_1$} ; %
        \node[latent, right=of x1] (x2) {$x_2$} ; %
        \node[latent, right=of x2] (x3) {$x_3$} ; %
        \node[latent, right=of x3] (x4) {$x_4$} ; %
        \node[latent, right=of x4] (x5) {$x_5$} ; %
        \node[obs, below=of x1] (y1) {$y_1$} ; %
        \node[obs, below=of x2] (y2) {$y_2$} ; %
        \node[obs, below=of x3] (y3) {$y_3$} ; %
        \node[obs, below=of x4] (y4) {$y_4$} ; %
        \node[latent, below=of x5] (y5) {$y_5$} ; %
        \edge {x1} {x2} ; %
        \edge {x2} {x3} ; %
        \edge {x3} {x4} ; %
        \edge {x4} {x5} ; %
        \edge {x1} {y1} ; %
        \edge {x2} {y2} ; %
        \edge {x3} {y3} ; %
        \edge {x4} {y4} ; %
        \edge {x5} {y5} ; %
      }
}
\end{center} \pause
\vspace{-0.2cm}
\begin{align}
p(\{x_t\}_{t=1}^5, \{y_t\}_{t=1}^5) = p(x_1) \left[\prod_{t=2}^5 p(x_t|x_{t-1})\right] \left[\prod_{t=1}^5 p(y_t|x_t)\right]
\end{align}
\end{frame}


\begin{frame}{Hidden Markov Models}
\begin{center}
\scalebox{0.7}{
      \tikz{ %
        \node[latent] (x1) {$x_1$} ; %
        \node[latent, right=of x1] (x2) {$x_2$} ; %
        \node[latent, right=of x2] (x3) {$x_3$} ; %
        \node[latent, right=of x3] (x4) {$x_4$} ; %
        \node[latent, right=of x4] (x5) {$x_5$} ; %
        \node[obs, below=of x1] (y1) {$y_1$} ; %
        \node[obs, below=of x2] (y2) {$y_2$} ; %
        \node[obs, below=of x3] (y3) {$y_3$} ; %
        \node[obs, below=of x4] (y4) {$y_4$} ; %
        \node[latent, below=of x5] (y5) {$y_5$} ; %
        \edge {x1} {x2} ; %
        \edge {x2} {x3} ; %
        \edge {x3} {x4} ; %
        \edge {x4} {x5} ; %
        \edge {x1} {y1} ; %
        \edge {x2} {y2} ; %
        \edge {x3} {y3} ; %
        \edge {x4} {y4} ; %
        \edge {x5} {y5} ; %
      }
}
\end{center}

Exercise: Is $y_i \ci y_j$ for $i\neq j$? \pause \\
\arrow No! $y_i$ is connected by an open path to $y_j$. \pause

Observe:
\begin{itemize}
\item We have specified a complex joint on observables $p(y_1,\dots,y_T)$. \pause
\item But \emph{simpler} structure than an ``always true'' factorisation, e.g.:
\begin{align}
p(y_1,\dots,y_T) \overset{\text{AT}}{=} p(y_1) p(y_2|y_1) p(y_3|y_2,y_1) \prod_{t=4}^T p(y_t|y_1,\dots,y_{t-1})
\end{align} \pause
\vspace{-0.3cm}
\item We never needed to specify a function on more than 2 variables! ($p(x_t|x_{t-1})$ vs e.g.~$p(y_t|y_1,\dots,y_{t-1})$).
\end{itemize}
\end{frame}


\begin{frame}{Hidden Markov Models: Example Applications}
\begin{itemize}
\item Figuring out the trajectory of a projectile from RADAR observations (e.g.~spacecraft).
\begin{itemize}
\item State is the location and speed of a projectile
\item $p(x_t|x_{t-1})$ is determined by Newton's laws, plus uncertainty form unknown forces acting on the object.
\item This is an example of physics-inspired model, where latent variables have physical interpretation.
\end{itemize}
\item Stock market models
\begin{itemize}
\item Posit some hypothetical ``market state'', which \textit{if you knew it} would predict the future well.
\item No ``real'' way in which the state ``exists'', but it can help with prediction!
\end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{Hidden Markov Models: Two Questions}
\begin{itemize}
\item Given all observations, what is our belief on the latent at time $t$? \\ I.e.~find $p(x_t|y_{1:t})$.
\item How can we predict into the future? \\ I.e.~find $p(y_{t+1}|y_{1:t})$.
\end{itemize}
\end{frame}


\begin{frame}{Filtering Distribution}
Systematic method: Probability of Everything%\footnote{Remember the goal: Write as distributions defined in specification of the model.}
\begin{align*}
p(x_t|y_{1:t}) &\stackrel{\text{AT}}{=} \int \frac{p(x_1,\dots,x_t,y_1,\dots,y_t)}{p(y_1,\dots,y_t)} \calcd x_1 \dots x_{t-1} \\
&\stackrel{\text{MA}}{=} \int \frac{p(x_1) \left[\prod_{t=2}^t p(x_t|x_{t-1})\right] \left[\prod_{t=1}^t p(y_t|x_t)\right]}{p(y_1,\dots,y_t)} \calcd x_1 \dots x_{t-1} \\
p(y_1,\dots,y_t) &= \int p(x_1) \left[\prod_{t=2}^t p(x_t|x_{t-1})\right] \left[\prod_{t=1}^t p(y_t|x_t)\right] \calcd x_1 \dots x_{t}
\end{align*}
\begin{itemize}
\item In principle, we could compute this (everything is specified in a density from the model specification!)
\item Lots of sums/integrals though! Very slow to compute!
\item Could rearrange the sums to speed things up?
\end{itemize}
\end{frame}


\begin{frame}{Recursive Filtering}
You can actually get a recursive equation to solve this problem, which is much easier to compute:
\begin{align*}
\text{Step 1:}&\quad &{}p(x_t|y_1,\dots,y_{t-1}) &= \int p(x_t|x_{t-1}) p(x_{t-1}|y_1,\dots,y_{t-1}) \calcd y_{1:t-1} \\
\text{Step 2:}&\quad &{} p(x_t|y_1,\dots,y_{t}) &= \frac{p(y_t|x_t)p(x_t|y_{1:t-1})}{p(y_t|y_{1:t-1})}
\end{align*}
\begin{itemize}
\item Only needs one sum/integral.
\item See \texttt{exercises.pdf} (question 9) for full derivation.
\item Look at \texttt{hmm-demo.ipynb} for alternative derivation and code demonstrating the model.
\end{itemize}
\end{frame}


\begin{frame}{HMM Demo}
Demo \texttt{hmm-demo.ipynb}. {\tiny TODO: Save figures from ipynb and put in slides.}
\end{frame}


\begin{frame}{Algorithms on Graphs}
\begin{itemize}
\item It took some effort to derive the recursive algorithm for efficient filtering.
\item Algorithms on graphs can discover these \emph{efficient} algorithms \emph{automatically}!
\item \arrow Belief Propagation (not examinable)
\end{itemize}
\end{frame}


\begin{frame}{Conclusion}
\begin{itemize}
\item Skill: Conditionals to Graphical Model
\item Skill: Graphical model to Conditionals
\item Skill: Conditional Independence from Graphical Model
\item Skill: Simplification of Posteriors using Conditional Independencies
\end{itemize}
\end{frame}


\begin{frame}{Further Reading}
  \begin{center}
    Bishop: Pattern Recognition and Machine Learning, Chapter 8 \\
    Directed graphical models
  \end{center}
\end{frame}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t,allowframebreaks]
\frametitle{References}
\linespread{1.0}
\tiny
\bibliographystyle{abbrv}
\bibliography{../includes/pi-literature.bib}
\end{frame}




\begin{frame}
\frametitle{}
\begin{center}
{\Large \emph{Not examinable from here}} \\
\end{center}
\end{frame}



\begin{frame}
\frametitle{}

\begin{center}
{\Large \emph{Factor Graphs}} \\
A different graphical representation
\end{center}
Good references:\\[3mm]
Kschischang et al.: Factor Graphs and the Sum-Product Algorithm. IEEE
Transactions on Information Theory (2001)\\[2mm]
Loeliger: An Introduction to Factor Graphs. IEEE Signal Processing
Magazine, (2004)\nocite{Kschischang2001,Loeliger2004}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Factor Graphs}

\begin{figure}
\centering
\includegraphics[height = 3cm]{./figures/factor_graph}
\end{figure}

\begin{itemize}
\item (Un)directed graphical models express a global function of
  several variables as a product of factors over subsets of those
  variables
\item Factor graphs make this decomposition explicit by introducing
  \cemph{additional nodes for the factors} themselves

\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Factorizing the Joint}
The joint distribution is a product of factors:
\begin{align*}
p(\vec x) = \prod_s f_s(\vec x_s)
\end{align*}

\begin{itemize}
\item $\vec x = (x_1,\dotsc, x_n)$
\item $\vec x_s$: Subset of variables
\item $f_s$: Factor; non-negative function of the variables $\vec x_s$
\end{itemize}

\pause
\begin{itemize}
\item Building a factor graph as a \cemph{bipartite graph:}
\begin{itemize}
\item Nodes for all random variables (same as in (un)directed
  graphical models)
\item Additional nodes for factors (black squares) in the joint
  distribution
\end{itemize}
\item Undirected links connecting each factor node to all of the
  variable nodes the factor depends on
\end{itemize}


\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example}

\begin{figure}
\centering
\includegraphics[height = 3cm]{./figures/fac_graph_ex1}
\end{figure}

\begin{align*}
p(\vec x) = f_a(x_1, x_2)f_b(x_1,x_2)f_c(x_2,x_3)f_d(x_3)
\end{align*}

\pause
\arrow Efficient inference algorithms for factor graphs (e.g.,
\cemph{sum-product algorithm})
\end{frame}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Directed Graphical Model $\to$ Factor Graph}

\begin{enumerate}
\item Take variable nodes from Bayesian network
\item Create additional factor nodes corresponding to the conditional distributions
\item Add appropriate links
\end{enumerate}

Not unique



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example: Directed Graph $\to$ Factor Graph}

\begin{figure}
\centering
\includegraphics[width = \hsize]{./figures/dgm2fac}
\end{figure}

\begin{itemize}
\item Directed graph with factorization $p(x_1)p(x_2)p(x_3|x_1,x_2)$
\item Factor graph with factor $f(x_1,x_2,x_3) = p(x_1)p(x_2)p(x_3|x_1,x_2)$
\item Factor graph with factors $f_a=p(x_1),~f_b = p(x_2),~f_c = p(x_3|x_1,x_2)$
\end{itemize}



\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Removing Cycles}

\begin{figure}
\centering
\includegraphics[width = 0.22\hsize,trim=0 4.2cm 10cm 0,clip]{./figures/remove_loops}
\includegraphics[width = 0.4\hsize,trim=7cm 2cm 0cm 2cm,clip]{./figures/remove_loops}
\end{figure}
\begin{equation}
p(x_3|x_2, x_1)p(x_2|x_1)p(x_1) = f_a(x_1, x_2, x_3)f_b(x_1, x_2)f_c(x_2) = f(x_1, x_2, x_3)
\end{equation}

\begin{itemize}
\item Local cycles in an (un)directed graph (due to links connecting
  parents of a node) can be removed on conversion to a factor graph
\end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{}

\begin{center}
{\Large \emph{Exact Inference in Factor Graphs}}

\end{center}
\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 

\begin{frame}
\frametitle{Sum-Product Algorithm for Factor Graphs}

\begin{itemize}
\item Factor graphs give a \cemph{uniform treatment to message
    passing}, which is used for inference in graphs
\item Inference: Find (marginal) posterior distributions
  \pause
\item Idea: \cemph{Local message passing} between nodes and factors
\item Two different types of messages: 
\begin{itemize}
\item Messages $\mu_{x\to f}(x)$ from variable nodes to factors
\item Messages $\mu_{f\to x}(x)$ from factors to variable nodes
\end{itemize}
\pause
\item Repeated sending of these messages through the graph converges
\item Factors transform messages into evidence for the receiving node
\end{itemize}

\end{frame}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{frame}
% \frametitle{Problem 1: Find a Marginal}

% \begin{itemize}
%   \item 
%  Assume all variables are hidden (for the time being)
% \item 
% By definition (sum rule): 
% \begin{align*}
% p(x) = \sum_{\vec x\backslash x}p(\vec x)
% \end{align*}
% \end{itemize}
% \end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Variable-to-Factor Message}

\begin{figure}
\centering
\includegraphics[height=3.5cm]{./figures/var2fac_msg}
\end{figure}
\vspace{-5mm}
\begin{align*}
\mu_{x_m\to f_s}(x_m) = \prod_{l\in\text{ne}(x_m)\backslash f_s}
\red{\mu_{f_l\to x_m}(x_m)}
\end{align*}

\begin{itemize}[<+->]
\item Take the product of all \red{incoming messages along all other links}
\item A variable node can send a message to a factor node once it has
  received messages from all other neighboring factors
\item The message that a node sends to a factor is made up of the
  messages that it receives from all other factors.
\end{itemize}

\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Factor-to-Variable Message}
\begin{figure}
\centering
\includegraphics[height=3cm]{./figures/fac2var_msg}
\end{figure}
\vspace{-5mm}

\begin{align*}
\mu_{f_s\to x}(x) = \onslide+<3->{\orange{\sum_{x_1}\cdots\sum_{x_M}}}\onslide+<2->{\blue{f_s(x, x_1,\dotsc, x_M)}}\red{\prod_{m\in\text{ne}(f_s)\backslash x}\mu_{x_m\to f_s}(x_m)}
\end{align*}

\begin{itemize}[<+->]
\item \red{Take the product of the incoming messages along all other links
  coming into the factor node}
\item \blue{Multiply by the factor associated with that node}
\item \orange{Marginalize over all variables associated with the
  \underline{incoming} messages}

\end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Initialization}

\begin{itemize}
\item 
If the leaf node is a \cemph{variable node}, initialize the corresponding messages
to 1:
\begin{align*}
\mu_{x\to f}(x) = 1
\end{align*}
\item If the leaf node is a \cemph{factor node}, the message should be
\begin{align*}
\mu_{f\to x}(x) = f(x)
\end{align*}
\end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example (1)}

\begin{columns}
\column{0.5\hsize}
\begin{figure}
\centering
\includegraphics[height = 3cm]{./figures/Figure8_52a}\\
\tiny{From PRML (Bishop, 2006)}
\end{figure}
\column{0.5\hsize}
\begin{align*}
  \mu_{x_1\to f_a}(x_1) &= 1\\
\mu_{f_a\to x_2}(x_2) &= \sum_{x_1} f_a(x_1,x_2)\cdot 1\\
\mu_{x_4\to f_c}(x_4) &= 1\\
\mu_{f_c\to x_2}(x_2)  &= \sum_{x_4} f_c(x_2,x_4)\cdot 1\\
\mu_{x_2\to f_b}(x_2) &= \mu_{f_a\to x_2} (x_2)\mu_{f_c\to x_2} (x_2)\\
\mu_{f_b\to x_3}(x_3) &= \sum_{x_2}f_b(x_2, x_3) \mu_{x_2\to f_b}(x_2)
\end{align*}
\end{columns}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Example (2)}

\begin{columns}
\column{0.5\hsize}
\begin{figure}
\centering
\includegraphics[height = 3cm]{./figures/Figure8_52b}\\
\tiny{From PRML (Bishop, 2006)}
\end{figure}
\column{0.5\hsize}
\begin{align*}
\mu_{x_3\to f_b}(x_3) &= 1
\\
\mu_{f_b\to x_2}(x_2) &= \sum_{x_3} f_b(x_2,x_3)\cdot 1
\\
\mu_{x_2\to f_a}(x_2) &= \mu_{f_b\to x_2}(x_2)\mu_{f_c\to x_2}(x_2)
\\
\mu_{f_a\to x_1}(x_1)  &= \sum_{x_2} f_a(x_1,x_2)\mu_{x_2\to f_a}(x_2)
\\
\mu_{x_2\to f_c}(x_2) &= \mu_{f_a\to x_2} (x_2)\mu_{f_b\to x_2} (x_2)
\\
\mu_{f_c\to x_4}(x_4) &= \sum_{x_2}f_c(x_2, x_4) \mu_{x_2\to f_c}(x_2)
\end{align*}
\end{columns}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Marginals}

\begin{figure}
\centering
\includegraphics[height = 5cm]{./figures/marginal_computation_fac}
\end{figure}

For a single variable node the marginal is given as the \cemph{product of all
incoming messages:}
\begin{align*}
\blue{p(x)} = \prod_{f_i\in\text{ne}(x)} \red{\mu_{f_i\to x}(x)}
\end{align*}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
\frametitle{Observed Variables \arrow Posterior}

\begin{itemize}
\item Thus far, we have focused on the case where all variables are
unobserved.
\item Posterior is always conditioned on observations
\item Partition $\vec x = \vec h \cup \vec v$, $\vec h$: hidden variables, $\vec
  v$: visible variables with observations $\hat {\vec v}$
\item $p(\vec v = \hat{\vec v}) = \prod_{i} I(v_i = \hat v_i)$
\item $p(\vec x) p(\vec v = \hat{\vec v}) = p(\vec h, \vec v =
  \hat{\vec v}) ~\propto~ p(\vec h|\vec v = \hat{\vec v})$
\item \cemph{Marginal posteriors} $p(h_i|\vec v = \hat{\vec v})$ can
  be obtained via sum-product algorithm and some local computations\\
  \arrow (Koller \& Friedman, 2009)\nocite{Koller2009}
 \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}{Exact Inference in (Un)Directed Graphical Models}

  \begin{itemize}
  \item Loops are possible \arrow \emph{Junction Tree Algorithm}
    (Lauritzen \& Spiegelhalter, 1988)\nocite{Lauritzen1988}
  \item Alternative: \emph{Loopy Belief Propagation} (Frey \& MacKay
    1998)\nocite{Frey1998}
  \end{itemize}
  
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
  \frametitle{Applications of Inference in Graphical Models}
\begin{figure}
\centering
\includegraphics[height = 2.5cm]{./figures/trueskill_example}
\hfill
\includegraphics[height = 2.5cm]{./figures/image_denoising_example}
\end{figure}

\begin{itemize}
\item \cemph{Ranking:} TrueSkill {\small (Herbrich et al., 2007)\nocite{Herbrich2007}}
\item \cemph{Computer vision:} de-noising, segmentation, semantic
  labeling, ... {\small{(e.g., Sucar \& Gillies, 1994; Shotton et al.,
    2006; Szeliski et al., 2008) }\nocite{Sucar1994,Shotton2006,Szeliski2008}}
\item \cemph{Coding theory:} Low-density parity-check codes, turbo
  codes, ... {\small{(e.g., McEliece et al., 1998)}\nocite{McEliece1998}}
\item \cemph{Linear algebra:} Solve linear equation systems
 {\small{(Shental et al., 2008)}\nocite{Shental2008}}
\item \cemph{Signal processing:} Iterative state estimation
  {\small{(e.g., Bickson et al., 2007; Deisenroth \& Mohamed,
      2012)}\nocite{Bickson2007,Deisenroth2012d}}
\end{itemize}

\end{frame}




\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
