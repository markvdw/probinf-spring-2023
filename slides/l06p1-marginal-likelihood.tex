%% Time-stamp: <2019-01-22 14:47:04 (marc)>
\documentclass[xcolor=x11names,compress, mathserif]{beamer}

\usepackage{../includes/MarkMathCmds}
\usepackage{animate}

\newcommand{\hackspace}{\hspace{4.2mm}}
\newcommand{\showstudent}[1]{}



% talk/author information
\newcommand{\authorname}{Mark van der Wilk}
\newcommand{\authoremail}{m.vdwilk@imperial.ac.uk}
\newcommand{\authoraffiliation}{
 Department of Computing\\Imperial
  College London}
\newcommand{\authortwitter}{markvanderwilk}
\newcommand{\slidesettitle}{\imperialBlue{Marginal likelihood}}
\newcommand{\footertitle}{Marginal likelihood}
\newcommand{\location}{Imperial College London}
\newcommand{\talkDate}{{February 3, 2023}}




\date{\imperialGray{\talkDate}}




% load defaults
\input{../includes/header.tex}
\input{../includes/titlepage.tex}


\begin{frame}{Learning objectives}
Previously we saw how the Bayesian framework tells us how to infer unseen parameters. Here we ask \textbf{why} it works.

We seek to answer:
\begin{itemize}
  \item What happens if we minimise the error to the training data?
  \item Does uncertainty prevent overfitting? If so, how?
  \item Why does the marginal likelihood prevent overfitting?
  \item What does the marginal likelihood measure?
\end{itemize}

\begin{center}
\tiny Use e.g.~Adobe Acrobat to view animations.
\end{center}
\end{frame}


\begin{frame}{Minimising training loss}
We're looking for a fit that will \emph{generalise} to new unseen test data.

Let's minimise the training loss of the posterior mean.
\begin{gather}
\mathcal{L}(\theta, \sigma) = \sum_{n=1}^N \left[ k_\theta(\vx_n, X) \left(\mathbf{K}_\theta + \sigma^2\mathbf{I}\right)\inv\vy  - y_n \right]^2 \\
\{\theta^*, \sigma^*\} = \argmin_{\theta, \sigma} \mathcal{L}(\theta, \sigma)
\end{gather}
\vspace{-0.3cm}
\animategraphics[loop,controls,width=\linewidth]{20}{./figures/marginal_likelihood/animation_mean_train/mean_train-}{0}{132}
\pause
\begin{center}
We can fit anything with a tiny lengthscale and noise variance!
\end{center}
\end{frame}



\begin{frame}{How does uncertainty help?}
Does uncertainty help against the overfitting?
\begin{figure}[T]
\onslide*<1>{\includegraphics[width=\textwidth]{./figures/marginal_likelihood/overfit-novar.png}}
\onslide*<2>{\includegraphics[width=\textwidth]{./figures/marginal_likelihood/overfit-var.png}}
\onslide*<3,4>{\includegraphics[width=\textwidth]{./figures/marginal_likelihood/overfit-var-testdata.png}}
\end{figure}
\pause
\pause
\pause
\begin{itemize}
\item Uncertainty by itself does not necessarily make predictions better, if the wrong model is chosen
\item Uncertainty does make predictions more cautious, which can be very useful!
\end{itemize}
\end{frame}





\begin{frame}{Marginal likelihood fixes things}
Instead, choose hyperparameters by maximising marginal likelihood:
\animategraphics[loop,controls,width=\linewidth]{20}{./figures/marginal_likelihood/animation_good_fit/good_fit-}{0}{101}
\vspace{-0.2cm}
\begin{center}
\tiny In above $\mathcal{L}$ is indicated by `datafit`, while `ELBO` indicates the marginal likelihood.
\end{center}
\begin{itemize}
  \item More sensible fit as the marginal likelihood rises
  \item Datafit gets worse!
\end{itemize}
\begin{center}
\Large Marginal likelihood trades off \\
\textbf{data fit} and \textbf{model complexity}.
\end{center}
\end{frame}


\begin{frame}{Why does marginal likelihood work?}
We have seen
\begin{itemize}
\item Minimising training error doesn't work
\item Uncertainty doesn't necessarily help, but does make us more cautious
\item Marginal likelihood seems to trade-off complexity and data fit
\end{itemize}

\begin{center}
\Large But \emph{why} does the marginal likelihood lead to models that generalise well?
\end{center}
\end{frame}



\begin{frame}[t]{Marginal likelihood as incremental prediction}
  We can split the marginal likelihood up using the \emph{product rule}:
  \begin{align}
    p(\vy\given\theta, X) &= p(y_1|\theta, \vx_1) p(y_2|\theta, \vx_1, y_1, \vx_2) p(y_3|\theta, \{\vx_i, y_i\}_{i=1}^2, \vx_3) \dots \\
                          &= \prod_{n=1}^N p(y_n|\theta, \{\vx_i, y_i\}_{i=1}^{n-1}, \vx_n)
  \end{align}
\pause
Remember
\begin{align*}
p(y_n|\theta, \{\vx_i, y_i\}_{i=1}^{n-1}, \vx_n) = \int p(y_n | f(\vx_n)) p( f(\vx_n) \given \{\vx_i, y_i\}_{i=1}^{n-1}, \vx_n) \calcd{f(\vx_n)}
\end{align*}
i.e.~the predictive distribution of $y_n$ based on the posterior given all points up to $n-1$.
\end{frame}


\begin{frame}{Marginal likelihood as incremental prediction}
  We can split the marginal likelihood up using the \emph{product rule}:
  \begin{align}
    p(\vy\given\theta, X) &= p(y_1|\theta, \vx_1) p(y_2|\theta, \vx_1, y_1, \vx_2) p(y_3|\theta, \{\vx_i, y_i\}_{i=1}^2, \vx_3) \dots \\
                          &= \prod_{n=1}^N p(y_n|\theta, \{\vx_i, y_i\}_{i=1}^{n-1}, \vx_n)
  \end{align}

  \vspace{0.5cm}

  \begin{itemize}
  \item The marginal likelihood measures how well previous training points predict the next one
  \item If it continuously predicted well on all $N$ points previously, it probably will do well next time
  \end{itemize}
\end{frame}


\begin{frame}{Marginal likelihood computation in action}
\animategraphics[loop,controls,width=\linewidth]{1.5}{./figures/marginal_likelihood/animation_incremental/short_lengthscale/fig}{0}{45}
\end{frame}


\begin{frame}{Marginal likelihood computation in action}
\animategraphics[loop,controls,width=\linewidth]{1.5}{./figures/marginal_likelihood/animation_incremental/long_lengthscale/fig}{0}{45}
\end{frame}


\begin{frame}{Marginal likelihood computation in action}
\animategraphics[loop,controls,width=\linewidth]{1.5}{./figures/marginal_likelihood/animation_incremental/opt_lengthscale/fig}{0}{45}
\end{frame}


\begin{frame}{Marginal likelihood evolution}
\begin{figure}[T]
\includegraphics[width=\textwidth]{./figures/marginal_likelihood/marglik-evolution.png}
\end{figure}
\vspace{-0.5cm}
\begin{itemize}
\item \textbf{Short} lengthscale consistently \emph{over-estimates variance}, so \emph{can't get a high density} even with the observation in the error bars 
\item \textbf{Long lengthscale} consistently \emph{under-estimates variance}, so gets a low density because the \emph{observations are outside error bars}
\item Optimal lengthscale \emph{trades off} these behaviours...\pause~well.
\end{itemize}
\end{frame}


\begin{frame}{Generalisation}
\begin{itemize}
\item A model with a high marginal likelihood is likely to \emph{generalise} well.
\item Its inductive bias has correctly predicted the next training point throughout the entire training set.
\item Marginal likelihoods are also related to \emph{generalisation error bounds}.
\end{itemize}
\pause
Generalisation error bounds state things like:
``With high probability, the error for method X on a test set will not be larger than Y''

\vspace{0.3cm}

PAC-Bayesian Theory Meets Bayesian Inference \citep{germain2016pac}
\end{frame}


\begin{frame}{Marginal likelihood as a prior probability}
A complementary view

\begin{itemize}
\item Marginal likelihood is the probability of the data under the prior.
\begin{align}
p(\vy|\theta, X) = \int p(\vy\given f(X), \theta) p(f(X)\given \theta) \calcd{f(X)}
\end{align}
\item For zero-mean GP regression models it has the explicit form:
\begin{align}
\log p(\vy|\theta, X) &= \log \NormDist{\vy; 0, \mathbf{K} + \sigma^2 \mathbf{I}} \\
&= -\frac{N}{2}\log 2\pi - \underbrace{\frac{1}{2}\log \detbar{\mathbf{K} +\sigma^2 \mathbf{I}}}_{\text{Complexity penalty}} - \underbrace{\frac{1}{2} \vy\transpose\left(\mathbf{K} + \sigma^2\mathbf{I}\right)\inv\vy}_{\text{Data fit}} \nonumber
\end{align}
\end{itemize}
\end{frame}


\begin{frame}{Complexity penalty and data fit}
\begin{align}
\log p(\vy|\theta, X) &= -\frac{N}{2}\log 2\pi - \underbrace{\frac{1}{2}\log \detbar{\mathbf{K} +\sigma^2 \mathbf{I}}}_{\text{Complexity penalty}} - \underbrace{\frac{1}{2} \vy\transpose\left(\mathbf{K} + \sigma^2\mathbf{I}\right)\inv\vy}_{\text{Data fit}}
\end{align}

\begin{itemize}
\item Determinant is product of eigenvalues (variances) of the covariance matrix -- the volume of the prior
\item Quadratic term measures whether the observation $\vy$ is within the variation allowed by the prior -- by lining $\vy$ up with the eigenvectors of the covariance
\end{itemize}

\end{frame}


\begin{frame}{Simple and complex models}
Probabilities have to normalise to 1, so a model \textbf{cannot} both
\begin{itemize}
\item be flexible enough to fit many datasets, and
\item make specific predictions after only a small amount of data.
\end{itemize}
\begin{figure}[T]
\includegraphics[width=0.5\textwidth]{./figures/marginal_likelihood/occam.png}
\end{figure}
\vspace{-0.7cm}
\begin{center}
\tiny From ``Occam's Razor'' -- Rasmussen \& Ghahramani (2000)
\end{center}
\begin{itemize}
\item Complex / flexible models spread their probability over many possible explanations of the data
\end{itemize}
\end{frame}

\begin{frame}{Occam's razor}
  \begin{center}
    \textit{``Entities are not to be multiplied without necessity''}\\
    or\\
    \textit{The simplest solution is most likely the right one} \\

    \vspace{1.0cm} \pause

    {\Large The marginal likelihood prefers the simplest model that still fits the data.}
  \end{center}
\pause
The marginal likelihood
\begin{itemize}
\item automatically penalises complex models, as the old adage states
\item comes from a principle as simple as representing your belief using probability
\item is automatically applied if you use Bayes' rule properly
\end{itemize}
\end{frame}


\begin{frame}{Marginal likelihood in action}
\animategraphics[loop,controls,width=\linewidth]{20}{./figures/marginal_likelihood/animation_periodic_fit/periodic_fit-}{0}{101}
\pause
\begin{itemize}
  \item Marginal likelihood learns \emph{how} to generalise
not just to fit the data.
  \item We chose the prior: $f(\vx) = \theta_sf_\text{smooth}(\vx) + \theta_pf_{\text{periodic}}(\vx)$, with smooth and periodic GP priors respectively.
  \item Amount of periodicity vs smoothness is automatically chosen by selecting hyperparameters $\theta_s, \theta_p$.
\end{itemize}
\end{frame}


\begin{frame}{Marginal likelihood in action}
\animategraphics[loop,controls,width=\linewidth]{1.5}{./figures/marginal_likelihood/animation_incremental/periodic/fig}{2}{45}
\end{frame}


\begin{frame}{Further reading}
\begin{itemize}
\item \nocite{itila} David~J.C.~MacKay. \textit{Information Theory, Inference, and Learning Algorithms}, chapter 28.
\end{itemize}
\end{frame}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% REFERENCES
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}[t,allowframebreaks]
\frametitle{References}
\linespread{1.0}
\tiny
\bibliographystyle{abbrv}
\bibliography{../includes/pi-literature.bib}
\end{frame}



\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
